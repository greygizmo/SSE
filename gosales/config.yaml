paths:
  raw: gosales/data/raw
  staging: gosales/data/staging
  curated: gosales/data/curated
  outputs: gosales/outputs
database:
  engine: azure          # sqlite | azure | auto
  sqlite_path: gosales/gosales.db
  # If true, require external DB; fail if AZSQL_* missing/unhealthy (no SQLite fallback)
  strict_db: false
  # Optional mapping of logical sources -> actual tables/views. Use 'csv' to read from a file.
  source_tables:
    sales_log: dbo.saleslog            # Azure SQL view for Sales Log
    industry_enrichment: csv           # 'csv' means read from ETL CSV path below
    # Assets & item taxonomy (Azure SQL views)
    customer_assets_rollups: dbo.customer_asset_rollups
    moneyball_assets: dbo.[Moneyball Assets]
    items_category_limited: dbo.items_category_limited
  # Optional explicit allow-list of DB objects used in dynamic SQL
  allowed_identifiers:
    - sales_log
    - industry_enrichment
    - dbo.saleslog
    - dbo.customer_asset_rollups
    - dbo.[Moneyball Assets]
    - dbo.items_category_limited
  # Where to write curated tables: 'db' (default, same engine) or 'sqlite'
  curated_target: sqlite
  curated_sqlite_path: gosales/gosales_curated.db
run:
  cutoff_date: "2024-12-31"
  prediction_window_months: 6
  lookback_years: 3
  # Optional explicit training cutoffs; when non-empty this overrides automatic generation.
  training_cutoffs: []
  # When `training_cutoffs` is empty, derive historical cutoffs stepping backwards by this many months.
  training_frequency_months: 6
  # Number of cutoffs to train (including the primary cutoff) when auto-generating.
  training_cutoff_count: 4
etl:
  coerce_dates_tz: "UTC"
  currency: "USD"
  fail_on_contract_breach: true
  allow_unknown_columns: false
  enable_industry_fuzzy: false
  fuzzy_min_unmatched: 50
  fuzzy_skip_if_coverage_ge: 0.95
  fuzzy_max_unmatched: 20000
  fuzzy_cache_path: gosales/outputs/industry_fuzzy_matches.csv
  # Path to CSV when industry_enrichment source is 'csv'
  industry_enrichment_csv: gosales/data/database_samples/TR - Industry Enrichment.csv
  # Exact DB column headers for identifiers
  source_columns:
    customer_id: CompanyId
    order_date: Rec_Date
    division: division
    customer_name: New_Business
  # Expand dim_customer roster: include NetSuite + Assets owners
  dim_customer:
    sources: [sales_log, ns, assets]
  assets:
    emit_debug_columns: true
logging:
  level: INFO
  jsonl: true
population:
  # Default: exclude prospects from training and scoring pipelines
  include_prospects: false
  # Define 'warm' as transactions within the last N months (train/scoring gating)
  warm_window_months: 18
  # If true, exclude prospects before heavy feature engineering to cut compute
  # (keeps only warm+cold customers in the roster during feature build)
  exclude_prospects_in_features: true
  # Which segments to include during feature-build: [warm], [cold], or [warm, cold]
  build_segments: [warm, cold]
labels:
  gp_min_threshold: 0.0
  denylist_skus_csv: gosales/data/lookup/label_denylist_skus.csv
features:
  windows_months: [3, 6, 12, 24]
  gp_winsor_p: 0.99
  add_missingness_flags: true
  use_eb_smoothing: true
  use_market_basket: true
  use_als_embeddings: false
  als_lookback_months: 24
  use_item2vec: false
  # Optional: run ALS over assets rollups (guarded sizes)
  use_assets_als: false
  assets_als:
    max_rows: 80000
    max_cols: 200
    factors: 16
    iters: 4
    reg: 0.1
  use_text_tags: false
  use_assets: true
  # Leakage guards
  expiring_guard_days: 30      # exclude [cutoff, cutoff+30d] from expiring_{30,60,90}d windows
  recency_floor_days: 30       # floor 'days_since_last_*' features to >=30 days
  recency_decay_half_lives_days: [30, 90, 180]
  enable_offset_windows: true
  offset_days: [60]
  enable_window_deltas: true
  affinity_lag_days: 60
  pooled_encoders_enable: true
  pooled_encoders_lookback_months: 24
  pooled_alpha_industry: 50.0
  pooled_alpha_sub: 50.0
  silence_pooled_encoder_warnings: true
  # Segment-specific feature allowlists (substring patterns). Kept alongside essentials
  # like customer_id and bought_in_division. Adjust as you iterate.
  segment_feature_allowlist:
    warm:
      - rfm__
      - recency
      - log_recency
      - decay
      - margin__
      - total_
      - avg_
      - tx_
      - gp_
      - xdiv__
      - diversity
      - enc__
      - is_
      - assets_
    cold:
      - assets_
      - enc__
      - is_
      - cust_cat_
      - cust_feat_
      - region
      - territory
      - industry
modeling:
  seed: 42
  folds: 3
  models: [lgbm, logreg]
  # Parallelism (LightGBM). Keep at 1 for strict determinism; LightGBM's
  # deterministic=true typically preserves determinism with >1 as well, but
  # tests assume single-thread unless you override here.
  n_jobs: 1
  lr_grid:
    l1_ratio: [0.0, 0.2, 0.5]
    C: [0.1, 1.0, 10.0]
  lgbm_grid:
    num_leaves: [31, 63]
    min_data_in_leaf: [50, 100]
    learning_rate: [0.05, 0.1]
    feature_fraction: [0.7, 0.9]
    bagging_fraction: [0.7, 0.9]
  calibration_methods: [platt, isotonic]
  top_k_percents: [5, 10, 20]
  capacity_percent: 10
  shap_max_rows: 50000
  # Cap rows used for final CV calibration to avoid multi-hour runs on huge
  # populations. 0 disables sampling (use all rows). When >0 and population
  # exceeds the cap, calibration runs on a stratified sample of this size.
  final_calibration_max_rows: 50000
  # Class imbalance controls
  class_weight: balanced        # balanced | none
  use_scale_pos_weight: true    # LightGBM scale_pos_weight toggle
  scale_pos_weight_cap: 10.0
  # Adopt SAFE feature policy for specific divisions (drops adjacency-heavy/short-window families)
  safe_divisions: ["Solidworks"]
  stability:
    coverage_floor: 0.0
    min_positive_rate: 0.0
    min_positive_count: 0
    penalty_lambda: 0.0
    cv_guard_max: null
    sparse_family_prefixes: []
    backbone_features: []
    backbone_prefixes: []

whitespace:
  weights: [0.60, 0.20, 0.10, 0.10]   # [p_icp_pct, lift_norm, als_norm, EV_norm]; non-negative, auto-normalized
  normalize: percentile               # percentile | pooled
  eligibility:
    exclude_if_owned_ever: true
    exclude_if_recent_contact_days: 21
    exclude_if_open_deal: true
    require_region_match: true
    exclude_if_active_assets: true
    reinclude_if_assets_expired_days: 365
  capacity_mode: top_percent          # top_percent | per_rep | hybrid
  accounts_per_rep: 25
  # Segment-aware allocation for capacity selection (warm/cold/prospect)
  segment_allocation:
    warm: 0.7
    cold: 0.3
    prospect: 0.0
  ev_cap_percentile: 0.95
  als_coverage_threshold: 0.30
  als_blend_weights: [0.5, 0.5]    # [transaction ALS, assets ALS]
  bias_division_max_share_topN: 0.6
  cooldown_days: 30
  cooldown_factor: 0.75
  # Optional challenger meta-learner over [p_icp_pct, lift_norm, als_norm, EV_norm]
  challenger_enabled: true
  challenger_model: lr
  # Emit legacy heuristic whitespace alongside Phase-4 outputs for comparison
  shadow_mode: false
  # Optional segment-wise weighting (e.g., industry, size_bin). Fallback to global when group < min_rows
  segment_columns: ["industry", "size_bin"]
  segment_min_rows: 250

validation:
  bootstrap_n: 1000
  top_k_percents: [5, 10, 20]
  capacity_grid: [5, 10, 20]
  ev_cap_percentile: 0.95
  segment_columns: [industry, industry_sub, region, territory]
  ks_threshold: 0.15
  psi_threshold: 0.25
  # Holdout label source: auto (prefer DB), db, or csv
  holdout_source: auto
  # Shift-14 Leakage Gauntlet thresholds
  shift14_epsilon_auc: 0.01
  shift14_epsilon_lift10: 0.25
  # Top-K ablation thresholds
  ablation_epsilon_auc: 0.01
  ablation_epsilon_lift10: 0.25
  # Gauntlet-only: subtract last N days from windowed aggregates
  gauntlet_mask_tail_days: 60
  # Gauntlet-only: embargo/purge days between train and validation folds
  gauntlet_purge_days: 60
  # Gauntlet-only: start labels at cutoff+buffer_days (horizon buffer)
  gauntlet_label_buffer_days: 30


